# Awesome Chinese LLM: A curated list of Chinese Large Language Model

# Datasets

* [NKCorpus](https://gitee.com/lidongwen1997/nkunlp-preprocessing) - 利用海量网络数据构建大型高质量中文数据集
* [BELLE](https://github.com/LianjiaTech/BELLE/tree/main/data/10M) - 10M中文数据集
* [MOSS](https://github.com/OpenLMLab/MOSS#%E6%95%B0%E6%8D%AE) - MOSS训练数据
* [Chinese Scientific Literature Dataset](https://github.com/ydli-ai/CSL) - A Large-scale Chinese Scientific Literature Dataset 中文科学文献数据集
* [CLUECorpus2020](https://github.com/CLUEbenchmark/CLUECorpus2020/) - 通过对Common Crawl的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料
* [News Commentary v13](https://github.com/dbiir/UER-py/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE)

# Pre-trained LLM

| Model | Size | Architecture | Repo/Chkpt | Paper | 
| ----- | ---- | ------------ | ----------- | ----- |
| 鹏程.盘古α | 13B | Decoder | [Github](https://github.com/huawei-noah/Pretrained-Language-Model) | [Paper](https://arxiv.org/pdf/2104.12369.pdf) |
| GLM | 130B | Decoder | [Github](https://github.com/THUDM/GLM-130B) | [Paper](https://arxiv.org/pdf/2210.02414.pdf) |
| MOSS | | | [Github](https://github.com/OpenLMLab/MOSS) |

# Instruction finetuned LLM
| Model | Size | Backbone | Repo/Chkpt | Paper | 
| ----- | ---- | ------------ | ----------- | ----- |
| Chinese-Vicuna | 7B | LlaMA | [Github](https://github.com/Facico/Chinese-Vicuna) | |
| BELLE | 7B, 13b | LlaMA | [Github](https://github.com/LianjiaTech/BELLE) |
| ChatGLM-6B| 6B | GLM | [Github](https://github.com/THUDM/ChatGLM-6B) |

